url_base_text <- "https://www.ajol.info/index.php/ajol/browseBy/alpha?letter=all"
url_base <- read_html(url_base_text)
library(ojsr)
library(tidyr)
library(dplyr)
library(stringr)
library(rvest)
library(httr)
url_base <- read_html(url_base_text)
url_journals <- url_base %>% html_nodes("br+ .action") %>% html_attr("href")
urls_jornals_text_archive2 <- paste0(url_journals, "/archive/2", sep = "")
urls_jornals_text_archive2[ <- paste0(url_journals, "/archive/2", sep = "")[1]
urls_jornals_text_archive2[1]
url <- read_html(urls_jornals_text_archive2[6])
urls_jornals_text_archive2[6]
urls_jornals_text_archive2 <- paste0(url_journals, "/issue/archive/2", sep = "")
url <- read_html(urls_jornals_text_archive2[6])
url <- read_html(urls_jornals_text_archive2[1])
urls_jornals_text_archive2[1]
href <- html_attr(html_element(url, ".title"), "href")
vec <- c(urls_jornals_text_archive2[i], href)
vec <- c(urls_jornals_text_archive2[1], href)
vec
urls_jornals_text_archive2 <- paste0(url_journals, "/issue/archive/2", sep = "")
urls_issues2 <- c()
for (i in 1:length(urls_jornals_text_archive2)) {
url <- read_html(urls_jornals_text_archive2[i])
href <- html_attr(html_element(url, ".title"), "href")
vec <- c(urls_jornals_text_archive2[1], href)
urls_issues2 <- data.frame(rbind(urls_issues2, vec))
}
get_issues2 <- function(x) {
url <- read_html(x)
href <- html_attr(html_element(url, ".title"), "href")
vec <- c(x, href)
urls_issues2 <- data.frame(rbind(urls_issues2, vec))
return(urls_issues2)
}
res <- lapply(urls_jornals_text_archive2[1:10], FUN = get_issues2
res <- lapply(urls_jornals_text_archive2[1:10], FUN = get_issues2)
View(res)
res[[6]][["X2"]]
res1 <- get_issues2(urls_jornals_text_archive2[6])
View(res1)
get_issues2 <- function(x) {
url <- read_html(x)
href <- html_attr(html_element(url, "a.title"), "href")
vec <- c(x, href)
urls_issues2 <- data.frame(rbind(urls_issues2, vec))
return(urls_issues2)
}
res1 <- get_issues2(urls_jornals_text_archive2[6])
View(res1)
get_issues2 <- function(x) {
url <- read_html(x)
href <- html_attr(html_elements(url, "a.title"), "href")
vec <- c(x, href)
urls_issues2 <- data.frame(rbind(urls_issues2, vec))
return(urls_issues2)
}
res1 <- get_issues2(urls_jornals_text_archive2[6])
?html_elements
url <- read_html(urls_jornals_text_archive2[6])
element <- html_elements(url, "a.title")
href <- html_attr(element, "href")
href
get_issues2 <- function(x) {
url <- read_html(x)
element <- html_elements(url, "a.title")
href <- html_attr(element, "href")
vec <- c(x, href)
urls_issues2 <- data.frame(rbind(urls_issues2, vec))
return(urls_issues2)
}
res1 <- get_issues2(urls_jornals_text_archive2[6])
url <- read_html(urls_jornals_text_archive2[6])
element <- html_elements(url, "a.title")
href <- html_attr(element, "href")
vec <- c(x, href)
vec <- c(urls_jornals_text_archive2[6], href)
get_issues2 <- function(x) {
url <- read_html(x)
element <- html_elements(url, "a.title")
href <- html_attr(element, "href")
return(href)
}
res1 <- get_issues2(urls_jornals_text_archive2[6])
res <- lapply(urls_jornals_text_archive2[1:10], FUN = get_issues2)
View(res)
library(parallel)
cores <- detectCores(logical = FALSE)
cores
cores <- detectCores()
library(h2o)
h2o.init()
res1 <- h2o.get_issues2(urls_jornals_text_archive2)
install.packages("foreach")
install.packages("doParallel")
library(foreach)
library(doParallel)
# Real physical cores in my computer
cores <- detectCores(logical = FALSE)
cl <- makeCluster(cores)
registerDoParallel(cl, cores=cores)
# clusterSplit are very convience to split data but it takes lots of extra memory
# chunks <- clusterSplit(cl, 1:len)
# split data by ourselves
chunk.size <- length(urls_jornals_text_archive2/cores
# clusterSplit are very convience to split data but it takes lots of extra memory
# chunks <- clusterSplit(cl, 1:len)
# split data by ourselves
chunk_size <- length(urls_jornals_text_archive2/cores)
# clusterSplit are very convience to split data but it takes lots of extra memory
# chunks <- clusterSplit(cl, 1:len)
# split data by ourselves
chunk_size <- length(urls_jornals_text_archive2)/cores
?foreach
urls_issues2 <- get_issues2(urls_jornals_text_archive2)
urls_issues2 <- sapply(urls_jornals_text_archive2, FUN = get_issues2)
h2o.shutdown()
h2o.shutdown()
urls_issues2 <- sapply(urls_jornals_text_archive2, FUN = get_issues2)
View(urls_issues2)
?lapply
url_issues2 <- unlist(urls_issues2)
urls_issues2 <- unlist(urls_issues2)
url_base_text <- "https://www.ajol.info/index.php/ajol/browseBy/alpha?letter=all"
url_base <- read_html(url_base_text)
url_journals <- url_base %>% html_nodes("br+ .action") %>% html_attr("href")
issues <- get_issues_from_archive(url_journals)
View(issues)
urls_issues2[1]
urls_jornals_text_archive3 <- paste0(url_journals, "/issue/archive/3", sep = "")
urls_issues3 <- sapply(urls_jornals_text_archive3, FUN = get_issues2)
urls_issues3 <- unlist(urls_issues3)
urls_jornals_text_archive4 <- paste0(url_journals, "/issue/archive/4", sep = "")
urls_issues4 <- sapply(urls_jornals_text_archive4, FUN = get_issues2)
urls_issues4 <- unlist(urls_issues4)
urls_jornals_text_archive5 <- paste0(url_journals, "/issue/archive/5", sep = "")
urls_issues5 <- sapply(urls_jornals_text_archive5, FUN = get_issues2)
urls_issues5 <- unlist(urls_issues5)
urls_issues5
urls_jornals_text_archive6 <- paste0(url_journals, "/issue/archive/6", sep = "")
urls_issues6 <- sapply(urls_jornals_text_archive6, FUN = get_issues2)
urls_issues6 <- unlist(urls_issues6)
urls_issues6
urls_jornals_text_archive7 <- paste0(url_journals, "/issue/archive/7", sep = "")
urls_issues7 <- sapply(urls_jornals_text_archive7, FUN = get_issues2)
urls_issues7 <- unlist(urls_issues7)
i
i
urls_jornals_text_archive8 <- paste0(url_journals, "/issue/archive/8", sep = "")
urls_issues8 <- sapply(urls_jornals_text_archive8, FUN = get_issues2)
urls_issues8 <- unlist(urls_issues8)
urls_issues8
urls_jornals_text_archive9 <- paste0(url_journals, "/issue/archive/9", sep = "")
urls_issues9 <- sapply(urls_jornals_text_archive9, FUN = get_issues2)
urls_issues9 <- unlist(urls_issues9)
urls_jornals_text_archive10 <- paste0(url_journals, "/issue/archive/10", sep = "")
urls_issues10 <- sapply(urls_jornals_text_archive10, FUN = get_issues2)
urls_issues10 <- unlist(urls_issues10)
urls_jornals_text_archive11 <- paste0(url_journals, "/issue/archive/11", sep = "")
urls_issues11 <- sapply(urls_jornals_text_archive11, FUN = get_issues2)
urls_issues11 <- unlist(urls_issues11)
urls_jornals_text_archive12 <- paste0(url_journals, "/issue/archive/12", sep = "")
urls_issues12 <- sapply(urls_jornals_text_archive12, FUN = get_issues2)
urls_issues12 <- unlist(urls_issues12)
vec_issues <- c(issues$output_url, urls_issues2, urls_issues3, urls_issues4, urls_issues5, urls_issues6, urls_issues7,urls_issues8, urls_issues9, urls_issues10, urls_issues11, urls_issues12)
articles <- get_articles_from_issue(vec_issues)
articles <- get_articles_from_issue(vec_issues)
?get_articles_from_issue
library(ojsr)
library(tidyr)
library(dplyr)
library(stringr)
library(rvest)
library(httr)
?get_articles_from_issue
library(ojsr)
write.csv(vec_issues, "url_issues.csv", row.names = FALSE)
install.packages("blogdown")
library(blogdown)
library(blogdown)
serve_site()
serve_site()
serve_site()
setwd("~/Documents/GitHub/my_website")
serve_site()
